{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fd79a-c7ba-45b4-9144-c5a7613dfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib\n",
    "!pip install opendatasets\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install rapidfuzz\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd61e7-b325-43f8-ab3a-6d78049d9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import FOAF, SKOS, RDF, RDFS, XSD, OWL\n",
    "import opendatasets as od\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15142776-4111-4fcd-b314-b9cf65ddf4be",
   "metadata": {},
   "source": [
    "## Download the datasets from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f207379-bbe9-4cf8-92e7-832f237db7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download('https://www.kaggle.com/datasets/nobelfoundation/nobel-laureates')\n",
    "od.download('https://www.kaggle.com/datasets/xabirhasan/journal-ranking-dataset')\n",
    "od.download('https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc39b1-f7d6-4b7e-bffc-79f64b558795",
   "metadata": {},
   "source": [
    "## Read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb7ed7-dc84-4245-a6cd-ccc8811c1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobels = pd.read_csv('nobel-laureates/archive.csv')\n",
    "nobels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f6c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = pd.read_csv('journal-ranking-dataset/journal_ranking_data.csv')\n",
    "journals.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a623c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('research-papers-dataset/dblp-v10.csv', index_col='id')\n",
    "papers.info()\n",
    "papers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fundings = pd.read_csv('budget_allocations.csv')\n",
    "fundings.info()\n",
    "fundings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159600cc-3679-4eb2-b3c1-3cc55269b0a5",
   "metadata": {},
   "source": [
    "## Parse our ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44be0f4-e6f3-4b2c-9fe2-4babce986daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "graph.parse('nobelOntology.ttl', format='turtle')\n",
    "graph.parse('http://eulersharp.sourceforge.net/2003/03swap/countries', format='turtle')\n",
    "\n",
    "NO = Namespace('http://www.semanticweb.org/a3d/ontologies/2024/10/nobelOntology/')\n",
    "JUR = Namespace('http://sweet.jpl.nasa.gov/2.3/humanJurisdiction.owl#')\n",
    "\n",
    "for ns_prefix, namespace in graph.namespaces():\n",
    "    print('{}: {}'.format(ns_prefix, namespace))\n",
    "\n",
    "for s, p, o in graph.triples((None, RDF.type, JUR.Country)):\n",
    "    print(f\"{s} is a country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cec06",
   "metadata": {},
   "source": [
    "## Shared handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, unicodedata, html, re\n",
    "\n",
    "def normalize_name(raw_name):\n",
    "    name = html.unescape(raw_name)\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('ascii')\n",
    "    uri_name = name.replace(',', '')\n",
    "    uri_name = uri_name.replace('.', '')\n",
    "    uri_name = uri_name.replace('-', ' ')\n",
    "    uri_name = uri_name.replace('\"', '')\n",
    "    uri_name = uri_name.replace('\\\\', '')\n",
    "    uri_name = uri_name.replace('/', '')\n",
    "    uri_name = ''.join(x for x in uri_name.title() if not x.isspace())\n",
    "    return (name, uri_name)\n",
    "\n",
    "country_mapping = {\n",
    "    'United States of America': 'United States',\n",
    "    'Scotland': 'United Kingdom',\n",
    "    'Northern Ireland': 'United Kingdom',\n",
    "    'Guadeloupe Island': 'Guadeloupe',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'East Germany': 'Germany',\n",
    "    'Federal Republic of Germany': 'Germany',\n",
    "    'Union of Soviet Socialist Republics': 'Russian Federation',\n",
    "    'Czechoslovakia': 'Czech Republic',\n",
    "    'Czechia': 'Czech Republic',\n",
    "    'Vietnam': 'Viet Nam',\n",
    "    'Russia': 'Russian Federation',\n",
    "    'Venezuela': 'Venezuela, Bolivarian Republic of',\n",
    "    'Taiwan': 'Taiwan, Province of China',\n",
    "    'Trinidad': 'Trinidad and Tobago',\n",
    "    'Iran': 'Iran, Islamic Republic of',\n",
    "    'Slovak Republic': 'Slovakia',\n",
    "    'Moldova': 'Moldova, Republic of',\n",
    "    'Libya': 'Libyan Arab Jamahiriya',\n",
    "    'Republic of Macedonia': 'Macedonia, the former Yugoslav Republic of',\n",
    "    'Macedonia': 'Macedonia, the former Yugoslav Republic of',\n",
    "    'South Korea': 'Korea, Republic of',\n",
    "    'Korea': 'Korea, Republic of',\n",
    "    \"People's Republic of China\": 'China',\n",
    "    'then Germany, now France': 'France',\n",
    "    \"Türkiye\": 'Turkey',\n",
    "    'Chinese Taipei': 'Taiwan, Province of China',\n",
    "}\n",
    "\n",
    "def handle_country(country_name):\n",
    "    \n",
    "    # Regex pattern to capture content inside parentheses\n",
    "    pattern = r\"\\((.*?)\\)\"\n",
    "    match = re.findall(pattern, country_name)\n",
    "    if len(match) != 0:\n",
    "        country_name = match[0]\n",
    "\n",
    "    # map country_name to its equivalent in Country namespace\n",
    "    country_name = country_mapping.get(country_name, country_name)\n",
    "    \n",
    "    country_query = f'''\n",
    "    SELECT ?country\n",
    "    WHERE {{\n",
    "        ?country rdf:type jur:Country;\n",
    "                foaf:name ?name.\n",
    "        FILTER(REGEX(?name, \"^{country_name}$\"))\n",
    "    }}'''\n",
    "\n",
    "    qres = graph.query(country_query)\n",
    "    if (len(qres) == 0):\n",
    "        print('Country not found: {}'.format(country_name))\n",
    "        return None\n",
    "\n",
    "    return qres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9785b-5d59-45b9-a29a-858309ef222d",
   "metadata": {},
   "source": [
    "## Populate the graph with nobel-laureates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc094af0-138d-41d4-9b69-f2f5816ba400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import topic_extraction\n",
    "\n",
    "def handle_city(raw_city):\n",
    "    (city_name, uri_city_name) = normalize_name(raw_city)\n",
    "    city = URIRef(NO[uri_city_name])\n",
    "    if (city, RDF.type, NO.City) not in graph: # new city\n",
    "        graph.add((city, RDF.type, NO.City))\n",
    "        graph.add((city, FOAF.name, Literal(city_name, datatype=XSD.string)))\n",
    "    return city\n",
    "\n",
    "def handle_org(raw_org):\n",
    "    (org_name, uri_org_name) = normalize_name(raw_org)\n",
    "    org = URIRef(NO[uri_org_name])\n",
    "    if (org, RDF.type, FOAF.Organization) not in graph: # new organization\n",
    "        graph.add((org, RDF.type, FOAF.Organization))\n",
    "        graph.add((org, FOAF.name, Literal(org_name, datatype=XSD.string)))\n",
    "    return org\n",
    "\n",
    "\n",
    "for index, row in nobels.iterrows():\n",
    "    nobel = URIRef(NO[row['Category'] + str(row['Year'])]) # the URI will be nobelNamespace + Category + Year\n",
    "    graph.add((nobel, RDF.type, NO.NobelPrize))\n",
    "    graph.add((nobel, NO.hasYear, Literal(row['Year'], datatype=XSD.gYear)))\n",
    "    graph.add((nobel, NO.hasNobelCategory, Literal(row['Category'], datatype=XSD.string)))\n",
    "\n",
    "    # handle Prize Share\n",
    "    prizeShare = str(row['Prize Share']).split('/')\n",
    "    if (nobel, NO.hasPrizeShare, None) not in graph:\n",
    "        graph.add((nobel, NO.hasPrizeShare, Literal(prizeShare[1], datatype=XSD.integer)))\n",
    "\n",
    "    # handle Motivation\n",
    "    if ((nobel, NO.hasMotivationTopics, None) not in graph) and (pd.notna(row['Motivation'])):\n",
    "        topics = topic_extraction.extract_topics(str(row['Motivation']), num_topics=1, num_words=5)\n",
    "        for idx, topic in enumerate(topics):\n",
    "            graph.add((nobel, NO.hasMotivationTopics, Literal(','.join(topic), datatype=XSD.string)))\n",
    "    \n",
    "    (laureate_name, uri_laureate_name) = normalize_name(str(row['Full Name']))\n",
    "    laureate = URIRef(NO[uri_laureate_name])\n",
    "    graph.add((laureate, FOAF.name, Literal(laureate_name, datatype=XSD.string)))\n",
    "\n",
    "    # handle Laureate Type\n",
    "    if row['Laureate Type'] == 'Organization':\n",
    "        graph.add((laureate, RDF.type, FOAF.Organization))\n",
    "    elif row['Laureate Type'] == 'Individual':\n",
    "        graph.add((laureate, RDF.type, FOAF.Person))\n",
    "\n",
    "    if pd.notna(row['Sex']):\n",
    "        graph.add((laureate, FOAF.gender, Literal(row['Sex'], datatype=XSD.string)))\n",
    "\n",
    "    graph.add((laureate, RDF.type, NO.Laureate))\n",
    "    graph.add((laureate, NO.hasWon, nobel))\n",
    "    \n",
    "    if pd.notna(row['Birth Date']):\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['Birth Date']), '%Y-%m-%d')\n",
    "            graph.add((laureate, NO.birthDate, Literal(row['Birth Date'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            splitted_date = str(row['Birth Date']).split('-')\n",
    "            new_date = splitted_date[0] + '-01-01'\n",
    "            print('Wrong Birth Date format in {}. The new date will be {}'.format(laureate, new_date))\n",
    "            graph.add((laureate, NO.birthDate, Literal(new_date, datatype=XSD.date)))\n",
    "\n",
    "    if pd.notna(row['Death Date']):\n",
    "        try:\n",
    "            datetime.datetime.strptime(str(row['Death Date']), '%Y-%m-%d')\n",
    "            graph.add((laureate, NO.deathDate, Literal(row['Death Date'], datatype=XSD.date)))\n",
    "        except ValueError:\n",
    "            splitted_date = str(row['Death Date']).split('-')\n",
    "            new_date = splitted_date[0] + '-01-01'\n",
    "            print('Wrong Death Date format in {}. The new date will be {}'.format(laureate, new_date))\n",
    "            graph.add((laureate, NO.deathDate, Literal(new_date, datatype=XSD.date)))\n",
    "\n",
    "    # handle birth city\n",
    "    if pd.notna(row['Birth City']):\n",
    "        birth_city = handle_city(str(row['Birth City']))\n",
    "        graph.add((laureate, NO.bornIn, birth_city))\n",
    "\n",
    "    # handle birth city country\n",
    "    if pd.notna(row['Birth Country']):\n",
    "        qres = handle_country(str(row['Birth Country']))\n",
    "        if (qres is not None) and ((birth_city, NO.locatedIn, None) not in graph): # new locatedIn\n",
    "            graph.add((birth_city, NO.locatedIn, next(iter(qres)).country)) # only the first match\n",
    "\n",
    "    # handle death city\n",
    "    if pd.notna(row['Death City']):\n",
    "        death_city = handle_city(str(row['Death City']))\n",
    "        graph.add((laureate, NO.diedIn, death_city))\n",
    "\n",
    "    # handle death city country\n",
    "    if pd.notna(row['Death Country']):\n",
    "        qres = handle_country(str(row['Death Country']))\n",
    "        if (qres is not None) and ((death_city, NO.locatedIn, None) not in graph): # new locatedIn\n",
    "            graph.add((death_city, NO.locatedIn, next(iter(qres)).country)) # only the first match\n",
    "\n",
    "    # handle organization\n",
    "    if pd.notna(row['Organization Name']):\n",
    "        org = handle_org(str(row['Organization Name']))\n",
    "        graph.add((laureate, NO.worksFor, org))\n",
    "\n",
    "        if pd.notna(row['Organization City']):\n",
    "            org_city = handle_city(str(row['Organization City']))\n",
    "            graph.add((org, NO.basedIn, org_city))\n",
    "\n",
    "        if pd.notna(row['Organization Country']):\n",
    "            qres = handle_country(str(row['Organization Country']))\n",
    "            if (qres is not None) and ((org_city, NO.locatedIn, None) not in graph): # new locatedIn\n",
    "                graph.add((org_city, NO.locatedIn, next(iter(qres)).country)) # only the first match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60a769-2771-40c0-b745-5eec80381c80",
   "metadata": {},
   "source": [
    "## Fix laureate type errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83e65f-e5db-49d3-8239-0396c6407cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, p, o in graph.triples((None, RDF.type, FOAF.Organization)):\n",
    "    if ((s, NO.bornIn, None)) in graph:\n",
    "        graph.add((s, RDF.type, FOAF.Person))\n",
    "        graph.remove((s, RDF.type, FOAF.Organization))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99100",
   "metadata": {},
   "source": [
    "## Populate the graph with journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_category(category):\n",
    "    # Remove numbers at the beginning, whitespaces, commas and the \"and\" word\n",
    "    cleaned = re.sub(r'^\\d+\\s+', '', category)\n",
    "    cleaned = cleaned.replace(',', '')\n",
    "    cleaned = cleaned.replace(' and ', ' ')\n",
    "    \n",
    "    # camelCase\n",
    "    parts = cleaned.split()\n",
    "    camel_case = parts[0].lower() + ''.join(word.capitalize() for word in parts[1:])\n",
    "    \n",
    "    return camel_case\n",
    "\n",
    "categories = [col for col in journals.columns[-27:]]\n",
    "\n",
    "for index, row in journals.iterrows():\n",
    "    journal_title, uri_journal_title = normalize_name(str(row['Title']))\n",
    "\n",
    "    if uri_journal_title == 'TelAviv':\n",
    "        uri_journal_title = 'Telaviv'\n",
    "\n",
    "    journal = URIRef(NO[uri_journal_title]) # URI will be nobelNamespace + Title\n",
    "    graph.add((journal, RDF.type, NO.Journal))\n",
    "    graph.add((journal, NO.hasTitle, Literal(journal_title, datatype=XSD.string)))\n",
    "    graph.add((journal, NO.hasHIndex, Literal(row['H-index'], datatype=XSD.integer)))\n",
    "    graph.add((journal, NO.hasSJR, Literal(row['SJR-index'], datatype=XSD.decimal)))\n",
    "    graph.add((journal, NO.hasOpenAccess, Literal(row['OA'], datatype=XSD.boolean)))\n",
    "\n",
    "    # Handle countries\n",
    "    qres = handle_country(str(row['Country']))\n",
    "    if (qres is not None) and ((journal, NO.hasCountry, None) not in graph): # new locatedIn\n",
    "        graph.add((journal, NO.hasCountry, next(iter(qres)).country)) # only the first match\n",
    "\n",
    "    # Handle categories\n",
    "    for category in categories:\n",
    "        if row[category] == 1:\n",
    "            graph.add((journal, NO.hasJournalCategory, NO[normalize_category(category)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b0114",
   "metadata": {},
   "source": [
    "## Populate the graph with fundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386eecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in fundings.iterrows():\n",
    "    funding_country, uri_funding_country = normalize_name(str(row['Reference area']))\n",
    "    funding = URIRef(uri_funding_country + str(row['TIME_PERIOD'])) # URI will be nobelNamespace + Country + Year\n",
    "    graph.add((funding, RDF.type, NO.Funding))\n",
    "    graph.add((funding, NO.hasYear, Literal(row['TIME_PERIOD'], datatype=XSD.gYear)))\n",
    "    graph.add((funding, NO.hasAmount, Literal(row['OBS_VALUE'], datatype=XSD.decimal)))\n",
    "\n",
    "    # Handle countries\n",
    "    qres = handle_country(str(row['Reference area']))\n",
    "    if (qres is not None) and ((None, NO.hasFunded, funding) not in graph): # new locatedIn\n",
    "        graph.add((next(iter(qres)).country, NO.hasFunded, funding)) # only the first match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec253baa-96c4-4cf8-9903-c049674704aa",
   "metadata": {},
   "source": [
    "## Populate graph with Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a02d53-86e9-45c0-8156-dc85fdad578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, Literal, RDF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def normalize_person_name(name):\n",
    "    return unidecode(name).lower().strip()\n",
    "\n",
    "# Function to find similar people in the graph\n",
    "def find_similar_person(new_name, graph, threshold=90):\n",
    "    new_name_normalized = normalize_person_name(new_name)\n",
    "    for person in graph.subjects(RDF.type, NO.Laureate):\n",
    "        existing_name = graph.value(person, FOAF.name)\n",
    "        if existing_name:\n",
    "            existing_name_normalized = normalize_person_name(str(existing_name))\n",
    "            similarity = fuzz.ratio(new_name_normalized, existing_name_normalized)\n",
    "            if similarity >= threshold:\n",
    "                return person\n",
    "    return None\n",
    "\n",
    "# Function to add researchers to the graph\n",
    "def add_researchers_to_graph(authors_list, graph):\n",
    "    for author in authors_list:\n",
    "        person_uri = find_similar_person(author, graph)\n",
    "        \n",
    "        if person_uri is None:\n",
    "            _, author_uri = normalize_name(author)\n",
    "            researcher = URIRef(NO[author_uri])\n",
    "            graph.add((researcher, RDF.type, FOAF.Person))\n",
    "            graph.add((researcher, FOAF.name, Literal(author, datatype=XSD.string)))\n",
    "\n",
    "            return researcher\n",
    "        else:\n",
    "            print(f\"Researcher {author} already exists as {person_uri}\")\n",
    "            return person_uri\n",
    "\n",
    "def find_or_create(graph, venue_name):\n",
    "    \"\"\"Ensure a venue exists in the graph, creating it if necessary.\"\"\"\n",
    "    CONFERENCE_KEYWORDS = ['workshop', 'conference', 'symposium']\n",
    "\n",
    "    venue_name, venue_uri = normalize_name(venue_name)\n",
    "    venue = URIRef(NO[venue_uri])\n",
    "\n",
    "    # Check if the venue already exists in the graph\n",
    "    if not (venue, RDF.type, NO.Journal) in graph and not (venue, RDF.type, NO.Conference) in graph:\n",
    "        print(f\"Creating {venue_uri}\")\n",
    "        # Determine type of venue based on name\n",
    "        if any(keyword in venue_name.lower() for keyword in CONFERENCE_KEYWORDS):\n",
    "            graph.add((venue, RDF.type, NO.Conference))\n",
    "        else:\n",
    "            graph.add((venue, RDF.type, NO.Journal))\n",
    "\n",
    "        graph.add((venue, NO.hasTitle, Literal(venue_name, datatype=XSD.string)))\n",
    "        \n",
    "    return venue\n",
    "\n",
    "# TODO: restringere il dataset con: researchers che sono laureates, venues che già esistono, N papers in più\n",
    "# FOR DEVELOPMENT\n",
    "papers = papers[:5000]\n",
    "\n",
    "for index, row in papers.iterrows():\n",
    "    paper = URIRef(NO[index])\n",
    "    graph.add((paper, RDF.type, NO.Paper))\n",
    "    graph.add((paper, NO.hasTitle, Literal(row['title'], datatype=XSD.string)))\n",
    "    graph.add((paper, NO.hasYear, Literal(row['year'], datatype=XSD.gYear)))\n",
    "    graph.add((paper, NO.hasCitations, Literal(row['n_citation'], datatype=XSD.integer)))\n",
    "\n",
    "    if pd.notna(row['abstract']):\n",
    "        topics = topic_extraction.extract_topics(row['abstract'], num_topics=1, num_words=5)\n",
    "        for idx, topic in enumerate(topics):\n",
    "            graph.add((paper, NO.hasAbstractTopics, Literal(','.join(topic), datatype=XSD.string)))\n",
    "\n",
    "    if pd.notna(row['venue']):\n",
    "        venue = find_or_create(graph, row['venue'])\n",
    "        graph.add((paper, NO.publishedIn, venue))\n",
    "\n",
    "    if pd.notna(row['authors']):\n",
    "        authors = row['authors'].replace('[', '').replace(']', '').replace('\\'', '').split(\", \")\n",
    "        researcher = add_researchers_to_graph(authors, graph)\n",
    "        graph.add((researcher, NO.hasWritten, paper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cc7ab-fcac-4c48-8a6d-4044cc0cf839",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.ttl', 'w', encoding='utf-8') as out:\n",
    "    out.write(graph.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
